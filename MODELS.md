# Gu√≠a de Modelos Ollama para el Sistema

## üöÄ Cambio R√°pido de Modelo

### Opci√≥n 1: Usando variable de entorno (temporal)
```bash
# Descargar el modelo
ollama pull qwen2.5:0.5b

# Ejecutar con el nuevo modelo
OLLAMA_MODEL=qwen2.5:0.5b python3 -m streamlit run app.py
```

### Opci√≥n 2: Archivo .env (permanente)
```bash
# Editar .env
echo "OLLAMA_MODEL=qwen2.5:0.5b" > .env

# Ejecutar normalmente
python3 -m streamlit run app.py
```

## üìä Comparativa de Modelos

### Ultra-R√°pidos (Recomendados para Codespaces)

**qwen2.5:0.5b** ‚ö° **[RECOMENDADO]**
- Tama√±o: ~0.4GB
- Velocidad: ~5-10 segundos en cpu/codespaces
- Calidad: Buena para soporte b√°sico
- Mejor para: Demos r√°pidos, pruebas, recursos limitados

**tinyllama**
- Tama√±o: ~0.6GB
- Velocidad: ~8-12 segundos en cpu/codespaces
- Calidad: Buena
- Mejor para: Balance velocidad/calidad

### R√°pidos

**phi3:mini**
- Tama√±o: ~2.3GB
- Velocidad: ~10-15 segundos en cpu/codespaces
- Calidad: Excelente
- Mejor para: Producci√≥n con recursos moderados

**llama3.2:1b**
- Tama√±o: ~1.3GB
- Velocidad: ~20-25 segundos en cpu/codespaces
- Calidad: Muy buena
- Mejor para: Balance general

### Alta Calidad (Requiere m√°s recursos)

**llama3.2:3b**
- Tama√±o: ~2GB
- Velocidad: ~30-40 segundos en cpu/codespaces
- Calidad: Excelente
- Mejor para: Desarrollo local

**llama3.1:8b**
- Tama√±o: ~5GB
- Velocidad: ~60-90 segundos en cpu/codespaces
- Calidad: Superior
- Mejor para: Producci√≥n con buenos recursos

## üîÑ Comandos √ötiles

```bash
# Listar modelos instalados
ollama list

# Descargar un modelo
ollama pull <nombre-modelo>

# Eliminar un modelo
ollama rm <nombre-modelo>

# Probar un modelo
ollama run <nombre-modelo> "Hola, ¬øc√≥mo est√°s?"

# Ver informaci√≥n del sistema
ollama show <nombre-modelo>
```

## üí° Recomendaciones

**Para GitHub Codespaces:**
1. **Primera opci√≥n:** `qwen2.5:0.5b` - Respuestas en ~5-10s
2. **Segunda opci√≥n:** `tinyllama` - Respuestas en ~8-12s
3. **Tercera opci√≥n:** `phi3:mini` - Respuestas en ~10-15s

**Para desarrollo local:**
1. **Primera opci√≥n:** `phi3:mini` - Excelente balance
2. **Segunda opci√≥n:** `llama3.2:3b` - Mejor calidad
3. **Tercera opci√≥n:** `llama3.1:8b` - M√°xima calidad

## üéØ Instalaci√≥n de Modelo Alternativo

```bash
# Descargar modelo ultra-r√°pido
ollama pull qwen2.5:0.5b

# Actualizar configuraci√≥n
echo "OLLAMA_MODEL=qwen2.5:0.5b" > .env

# Probar el modelo
python3 test_ollama_response.py

# Ejecutar aplicaci√≥n
python3 -m streamlit run app.py
```

## ‚ö†Ô∏è Notas

- Los tiempos son aproximados y dependen de los recursos del sistema
- Los modelos m√°s peque√±os son m√°s r√°pidos pero pueden tener menor calidad
- Para producci√≥n, considera usar OpenAI API en lugar de Ollama local
- En Codespaces, se recomienda usar modelos ‚â§ 2GB para mejor rendimiento
