import subprocess
import time
import requests
import os
import sys
from typing import Optional

class OllamaManager:
    """Gestor del ciclo de vida del servidor Ollama y disponibilidad de modelos."""

    def __init__(self, base_url: str = "http://localhost:11434", timeout: int = 30):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.process: Optional[subprocess.Popen] = None

    def _install_ollama_binary(self) -> bool:
        """Instala Ollama usando el script oficial (mÃ©todo comprobado para Codespaces).
        Basado en: https://github.com/BlackTechX011/Ollama-in-GitHub-Codespaces
        """
        try:
            print("ğŸ“¦ Instalando Ollama con el instalador oficial...")
            
            # Descargar e instalar usando el script oficial
            result = subprocess.run(
                "curl -fsSL https://ollama.com/install.sh | sh",
                shell=True,
                capture_output=True,
                text=True,
                timeout=300  # 5 minutos
            )
            if result.returncode != 0:
                print("âŒ Error instalando Ollama")
                if result.stderr:
                    print(f"   Error: {result.stderr[:200]}")
                return False
            
            # Verificar instalaciÃ³n
            result = subprocess.run(
                ["ollama", "--version"],
                capture_output=True,
                timeout=5
            )
            if result.returncode == 0:
                print("âœ… Ollama instalado correctamente")
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ Error en instalaciÃ³n: {e}")
            return False

    def _run_setup_if_needed(self) -> bool:
        """Ejecuta la instalaciÃ³n automÃ¡tica si Ollama no estÃ¡ disponible."""
        try:
            # Verificar si ollama estÃ¡ instalado
            subprocess.run(["ollama", "--version"], capture_output=True, check=True, timeout=5)
            return True  # Ollama disponible
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
            pass  # Ollama no disponible, intentar instalaciÃ³n

        print("ğŸ”§ Ollama no encontrado. Iniciando instalaciÃ³n automÃ¡tica...")
        
        # Intentar instalaciÃ³n con el mÃ©todo binario oficial
        if self._install_ollama_binary():
            return True
        
        # Si falla, intentar script de instalaciÃ³n
        try:
            install_script = os.path.join(os.path.dirname(__file__), "install_ollama.py")
            if os.path.exists(install_script):
                print("ğŸ“¥ Intentando con script de instalaciÃ³n...")
                result = subprocess.run(
                    [sys.executable, install_script],
                    capture_output=True,
                    text=True,
                    timeout=600
                )
                if result.returncode == 0:
                    print("âœ… Ollama instalado correctamente")
                    return True
        except Exception as e:
            print(f"âŒ Error ejecutando script de instalaciÃ³n: {e}")
        
        print("âŒ InstalaciÃ³n automÃ¡tica fallÃ³")
        print("ğŸ“‹ Instala manualmente: curl -fsSL https://ollama.com/install.sh | sh")
        return False

    def is_running(self) -> bool:
        """Check if Ollama server is running."""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except requests.RequestException:
            return False

    def start_server(self) -> bool:
        """Inicia el servidor Ollama si no estÃ¡ ejecutÃ¡ndose."""
        if self.is_running():
            print("âœ… Servidor Ollama ya estÃ¡ ejecutÃ¡ndose")
            return True

        try:
            print("ğŸ–¥ï¸  Iniciando servidor Ollama...")
            self.process = subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                start_new_session=True
            )

            # Esperar a que el servidor inicie
            start_time = time.time()
            while time.time() - start_time < self.timeout:
                if self.is_running():
                    print("âœ… Servidor Ollama iniciado correctamente")
                    return True
                time.sleep(1)

            print("âŒ El servidor no iniciÃ³ en el tiempo esperado")
            self.stop_server()
            return False

        except FileNotFoundError:
            print("âŒ Comando 'ollama' no encontrado. Intentando instalaciÃ³n automÃ¡tica...")
            if self._run_setup_if_needed():
                # Reintentar despuÃ©s de la instalaciÃ³n
                return self.start_server()
            else:
                print("âŒ InstalaciÃ³n automÃ¡tica fallÃ³")
                print("ğŸ“‹ Instala manualmente: curl -fsSL https://ollama.com/install.sh | sh")
                return False
        except Exception as e:
            print(f"âŒ Error iniciando servidor Ollama: {e}")
            return False

    def stop_server(self):
        """Stop the Ollama server if we started it."""
        if self.process:
            try:
                if os.name == 'nt':
                    self.process.terminate()
                else:
                    os.killpg(os.getpgid(self.process.pid), 15)  # SIGTERM
                self.process.wait(timeout=10)
                print("Ollama server stopped.")
            except subprocess.TimeoutExpired:
                self.process.kill()
                print("Ollama server force killed.")
            except Exception as e:
                print(f"Error stopping Ollama server: {e}")
            finally:
                self.process = None

    def model_available(self, model_name: str) -> bool:
        """Verifica si un modelo especÃ­fico estÃ¡ disponible."""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                return any(model['name'] == model_name for model in models)
            return False
        except requests.RequestException:
            return False

    def pull_model(self, model_name: str) -> bool:
        """Descarga un modelo si no estÃ¡ disponible."""
        if self.model_available(model_name):
            print(f"âœ… Modelo '{model_name}' ya estÃ¡ disponible")
            return True

        try:
            print(f"ğŸ“¥ Descargando modelo '{model_name}'...")
            print("â³ Esto puede tomar varios minutos...")
            result = subprocess.run(
                ["ollama", "pull", model_name],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutos
            )

            if result.returncode == 0:
                print(f"âœ… Modelo '{model_name}' descargado correctamente")
                return True
            else:
                print(f"âŒ Error descargando modelo '{model_name}': {result.stderr}")
                return False

        except subprocess.TimeoutExpired:
            print(f"â° Tiempo agotado descargando modelo '{model_name}'")
            return False
        except FileNotFoundError:
            print("âŒ Comando 'ollama' no encontrado. Intentando instalaciÃ³n automÃ¡tica...")
            if self._run_setup_if_needed():
                # Reintentar despuÃ©s de la instalaciÃ³n
                return self.pull_model(model_name)
            else:
                print("âŒ InstalaciÃ³n automÃ¡tica fallÃ³")
                print("ğŸ“‹ Instala manualmente: curl -fsSL https://ollama.com/install.sh | sh")
                return False
        except Exception as e:
            print(f"âŒ Error descargando modelo: {e}")
            return False

    def ensure_ready(self, model_name: str) -> bool:
        """Asegura que el servidor Ollama estÃ© ejecutÃ¡ndose y el modelo disponible."""
        print(f"\nğŸ¤– Preparando Ollama con modelo '{model_name}'...")
        
        if not self.start_server():
            return False

        if not self.pull_model(model_name):
            return False

        print("âœ… Ollama listo para usar\n")
        return True

# Instancia global del gestor
_manager = None

def get_ollama_manager(base_url: str = "http://localhost:11434") -> OllamaManager:
    """Obtiene o crea la instancia global del gestor Ollama."""
    global _manager
    if _manager is None:
        _manager = OllamaManager(base_url)
    return _manager

def ensure_ollama_ready(model_name: str, base_url: str = "http://localhost:11434") -> bool:
    """FunciÃ³n de conveniencia para asegurar que Ollama estÃ© listo con el modelo especificado."""
    manager = get_ollama_manager(base_url)
    return manager.ensure_ready(model_name)

if __name__ == "__main__":
    # Probar el gestor
    model_name = os.getenv('OLLAMA_MODEL', 'qwen2.5:0.5b')
    print("ğŸ§ª Probando gestor de Ollama...")
    if ensure_ollama_ready(model_name):
        print("ğŸ‰ Â¡Ollama estÃ¡ listo!")
    else:
        print("âŒ FallÃ³ la preparaciÃ³n de Ollama")
        sys.exit(1)