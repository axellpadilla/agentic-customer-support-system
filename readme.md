# Agentic Customer Support System

Sistema inteligente de atenciÃ³n al cliente que usa agentes de IA para proporcionar respuestas contextuales y empÃ¡ticas. Construido con Python, Pydantic-AI, Streamlit, y modelos locales Ollama con compatibilidad OpenAI.

## ğŸŒŸ CaracterÃ­sticas

- **GestiÃ³n AutomÃ¡tica de Ollama**: Inicia automÃ¡ticamente el servidor Ollama y descarga modelos segÃºn sea necesario
- **Contexto Inteligente**: Mantiene historial y preferencias del cliente
- **Servicio por Niveles**: Diferentes niveles de servicio para diferentes categorÃ­as de clientes
- **Seguimiento de Pedidos**: InformaciÃ³n en tiempo real del estado de los pedidos
- **AnÃ¡lisis de Sentimiento**: Analiza el sentimiento del cliente para mejores respuestas
- **Base de Conocimiento**: Acceso rÃ¡pido a polÃ­ticas de envÃ­o, devoluciones y garantÃ­as
- **Manejo de Errores**: DegradaciÃ³n elegante cuando los servicios no estÃ¡n disponibles

## ğŸ› ï¸ Stack TecnolÃ³gico

- Python 3.8+
- Streamlit
- Pydantic
- Pydantic-AI
- Ollama (modelos locales con compatibilidad OpenAI API)

## ğŸ“‹ Requisitos

- Python 3.8 o superior
- Git

## ğŸš€ InstalaciÃ³n

### MÃ©todo 1: GitHub Codespaces (AutomÃ¡tico) â­

Si abres este repositorio en GitHub Codespaces, **todo se configura automÃ¡ticamente**:
- Instala todas las dependencias
- Configura Ollama
- Descarga el modelo

Solo necesitas ejecutar:
```bash
python -m streamlit run app.py
```

### MÃ©todo 2: Inicio RÃ¡pido (Recomendado para local)

1. **Clonar el repositorio:**
```bash
git clone https://github.com/Croups/agentic-customer-support-system
cd agentic-customer-support-system
```

2. **Instalar dependencias:**
```bash
pip install -r requirements.txt
```

3. **Iniciar la aplicaciÃ³n:**
```bash
python -m streamlit run app.py
```

**Â¡Eso es todo!** El sistema instalarÃ¡ y configurarÃ¡ Ollama automÃ¡ticamente la primera vez que lo ejecutes.

### MÃ©todo 3: Script de Inicio RÃ¡pido

Un solo comando para configurar y ejecutar todo:

```bash
chmod +x start.sh && ./start.sh
```

Este script ejecuta automÃ¡ticamente `setup.sh` para configurar el entorno completo.

### MÃ©todo 4: Solo ConfiguraciÃ³n (sin ejecutar)

Si prefieres configurar todo primero y ejecutar despuÃ©s:

```bash
# Configurar todo (instala dependencias, Ollama y descarga modelo)
chmod +x setup.sh && ./setup.sh

# Luego ejecutar cuando quieras
python -m streamlit run app.py
```

### MÃ©todo 5: ConfiguraciÃ³n Manual

```bash
# Instalar dependencias
pip install -r requirements.txt

# Instalar Ollama
python install_ollama.py

# Verificar instalaciÃ³n
python test_installation.py

# Ejecutar app
python -m streamlit run app.py
```

## ğŸ”§ ConfiguraciÃ³n (Opcional)

El sistema soporta mÃºltiples proveedores de LLM con detecciÃ³n automÃ¡tica:

### OpciÃ³n 1: GitHub Models (Recomendado) ğŸŒŸ

Usa modelos gratuitos de GitHub con tu Personal Access Token:

```bash
# Crear .env con siguiendo .env.example

**Ventajas:**
- âœ… Gratis para uso personal hasta 15 solicitudes por minuto
- âœ… Ultra-rÃ¡pido (~1-3 segundos)
- âœ… Modelos de alta calidad
- âœ… Sin instalaciÃ³n local necesaria

**Crear token:** [github.com/settings/tokens](https://github.com/settings/tokens)

**Modelos disponibles:**
[github.com/marketplace/models](https://github.com/marketplace/models)

### OpciÃ³n 3: Ollama Local (Por Defecto)

```bash
OLLAMA_MODEL=qwen2.5:0.5b  # Modelo ultra-rÃ¡pido (recomendado para Codespaces)
```
### Modelos Recomendados por Velocidad

**Para GitHub Codespaces (recursos limitados):**
- `qwen2.5:0.5b` - ~0.4GB, **ultra-rÃ¡pido** (~5-10s), ideal para demos âš¡
- `tinyllama` - ~0.6GB, **muy rÃ¡pido** (~8-12s), buena calidad
- `phi3:mini` - ~2.3GB, rÃ¡pido (~10-15s), excelente para producciÃ³n
- `llama3.2:1b` - ~1.3GB, balanceado (~20-25s) (por defecto)

**Para desarrollo local (mÃ¡s recursos):**
- `phi3` - ~2.3GB, rÃ¡pido y eficiente
- `llama3.2:3b` - ~2GB, rendimiento balanceado
- `llama3.1:8b` - ~5GB, mejor calidad

**Cambiar modelo:**
```bash
# Descargar modelo mÃ¡s rÃ¡pido
ollama pull qwen2.5:0.5b

# Actualizar variable de entorno
echo "OLLAMA_MODEL=qwen2.5:0.5b" > .env

# O exportar temporalmente
export OLLAMA_MODEL=qwen2.5:0.5b
```

## ğŸ’» Uso

Una vez iniciada la aplicaciÃ³n con `python -m streamlit run app.py`:

1. Abre tu navegador en `http://localhost:8501`
2. Usa la interfaz para:
   - Ver informaciÃ³n del cliente
   - Consultar estado de pedidos
   - Acceder a informaciÃ³n de envÃ­os
   - Ver polÃ­ticas de devoluciÃ³n
   - Obtener informaciÃ³n de garantÃ­as

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ app.py                # Interfaz Streamlit
â”œâ”€â”€ support_system.py     # Sistema de agentes principal
â”œâ”€â”€ ollama_manager.py     # GestiÃ³n del servidor y modelos Ollama
â”œâ”€â”€ install_ollama.py     # Instalador automÃ¡tico de Ollama
â”œâ”€â”€ requirements.txt      # Dependencias del proyecto
â””â”€â”€ README.md            # DocumentaciÃ³n
```

## ğŸ”„ Prioridad de Proveedores

El sistema detecta automÃ¡ticamente quÃ© proveedor usar en este orden:

1. **GitHub Models** (si `LLM_TOKEN`, `LLM_ENDPOINT`, `LLM_MODEL` estÃ¡n definidos)
2. **OpenAI API** (si `USE_OPENAI=true` y `OPENAI_API_KEY` estÃ¡ definido)  
3. **Ollama Local** (por defecto, usa `OLLAMA_MODEL`)

**Comparativa:**

| Proveedor | Velocidad | Costo | InstalaciÃ³n | Recomendado para |
|-----------|-----------|-------|-------------|------------------|
| **GitHub Models** ğŸŒŸ | âš¡âš¡âš¡ Ultra-rÃ¡pido | Gratis | Ninguna | Codespaces, desarrollo |
| **OpenAI API** | âš¡âš¡âš¡ Ultra-rÃ¡pido | De pago | Ninguna | ProducciÃ³n |
| **Ollama Local** | âš¡ RÃ¡pido | Gratis | Requerida | Local, privacidad |

## ğŸ”§ InstalaciÃ³n Manual de Ollama

Si prefieres instalar Ollama manualmente:

```bash
# MÃ©todo recomendado (script oficial)
curl -fsSL https://ollama.com/install.sh | sh

# Iniciar servidor
ollama serve

# Descargar modelo (en otra terminal)
ollama pull llama3.2:1b
```

## âš ï¸ Consideraciones para Codespaces

- **LÃ­mites de recursos**: Los Codespaces tienen lÃ­mites de CPU/memoria que pueden afectar el rendimiento
- **Almacenamiento**: Los modelos se descargan al almacenamiento del Codespace
- **Red**: La descarga inicial del modelo requiere conexiÃ³n a internet
- **Persistencia**: Los modelos persisten entre sesiones pero pueden necesitar re-descarga si se reinicia el entorno

# Modificado de repositorio original como demo:
https://github.com/Croups/agentic-customer-support-system


