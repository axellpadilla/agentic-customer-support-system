# Agentic Customer Support System

Sistema inteligente de atenciÃ³n al cliente que usa agentes de IA para proporcionar respuestas contextuales y empÃ¡ticas. Construido con Python, Pydantic-AI, Streamlit, y modelos locales Ollama con compatibilidad OpenAI.

## ğŸŒŸ CaracterÃ­sticas

- **GestiÃ³n AutomÃ¡tica de Ollama**: Inicia automÃ¡ticamente el servidor Ollama y descarga modelos segÃºn sea necesario
- **Contexto Inteligente**: Mantiene historial y preferencias del cliente
- **Servicio por Niveles**: Diferentes niveles de servicio para diferentes categorÃ­as de clientes
- **Seguimiento de Pedidos**: InformaciÃ³n en tiempo real del estado de los pedidos
- **AnÃ¡lisis de Sentimiento**: Analiza el sentimiento del cliente para mejores respuestas
- **Base de Conocimiento**: Acceso rÃ¡pido a polÃ­ticas de envÃ­o, devoluciones y garantÃ­as
- **Manejo de Errores**: DegradaciÃ³n elegante cuando los servicios no estÃ¡n disponibles

## ğŸ› ï¸ Stack TecnolÃ³gico

- Python 3.8+
- Streamlit
- Pydantic
- Pydantic-AI
- Ollama (modelos locales con compatibilidad OpenAI API)

## ğŸ“‹ Requisitos

- Python 3.8 o superior
- Git

## ğŸš€ InstalaciÃ³n

### MÃ©todo 1: GitHub Codespaces (AutomÃ¡tico) â­

Si abres este repositorio en GitHub Codespaces, **todo se configura automÃ¡ticamente**:
- Instala todas las dependencias
- Configura Ollama
- Descarga el modelo

Solo necesitas ejecutar:
```bash
python -m streamlit run app.py
```

### MÃ©todo 2: Inicio RÃ¡pido (Recomendado para local)

1. **Clonar el repositorio:**
```bash
git clone https://github.com/Croups/agentic-customer-support-system
cd agentic-customer-support-system
```

2. **Instalar dependencias:**
```bash
pip install -r requirements.txt
```

3. **Iniciar la aplicaciÃ³n:**
```bash
python -m streamlit run app.py
```

**Â¡Eso es todo!** El sistema instalarÃ¡ y configurarÃ¡ Ollama automÃ¡ticamente la primera vez que lo ejecutes usando el [mÃ©todo comprobado de instalaciÃ³n binaria](https://github.com/BlackTechX011/Ollama-in-GitHub-Codespaces).

### MÃ©todo 3: Script de Inicio RÃ¡pido

Un solo comando para configurar y ejecutar todo:

```bash
chmod +x start.sh && ./start.sh
```

Este script ejecuta automÃ¡ticamente `setup.sh` para configurar el entorno completo.

### MÃ©todo 4: Solo ConfiguraciÃ³n (sin ejecutar)

Si prefieres configurar todo primero y ejecutar despuÃ©s:

```bash
# Configurar todo (instala dependencias, Ollama y descarga modelo)
chmod +x setup.sh && ./setup.sh

# Luego ejecutar cuando quieras
python -m streamlit run app.py
```

### MÃ©todo 5: ConfiguraciÃ³n Manual

```bash
# Instalar dependencias
pip install -r requirements.txt

# Instalar Ollama
python install_ollama.py

# Verificar instalaciÃ³n
python test_installation.py

# Ejecutar app
python -m streamlit run app.py
```

## ğŸ”§ ConfiguraciÃ³n (Opcional)

Puedes personalizar el modelo de Ollama creando un archivo `.env`:

```bash
OLLAMA_MODEL=qwen2.5:0.5b  # Modelo ultra-rÃ¡pido (recomendado para Codespaces)
```

### Modelos Recomendados por Velocidad

**Para GitHub Codespaces (recursos limitados):**
- `qwen2.5:0.5b` - ~0.4GB, **ultra-rÃ¡pido** (~5-10s), ideal para demos âš¡
- `tinyllama` - ~0.6GB, **muy rÃ¡pido** (~8-12s), buena calidad
- `phi3:mini` - ~2.3GB, rÃ¡pido (~10-15s), excelente para producciÃ³n
- `llama3.2:1b` - ~1.3GB, balanceado (~20-25s) (por defecto)

**Para desarrollo local (mÃ¡s recursos):**
- `phi3` - ~2.3GB, rÃ¡pido y eficiente
- `llama3.2:3b` - ~2GB, rendimiento balanceado
- `llama3.1:8b` - ~5GB, mejor calidad

**Cambiar modelo:**
```bash
# Descargar modelo mÃ¡s rÃ¡pido
ollama pull qwen2.5:0.5b

# Actualizar variable de entorno
echo "OLLAMA_MODEL=qwen2.5:0.5b" > .env

# O exportar temporalmente
export OLLAMA_MODEL=qwen2.5:0.5b
```

ğŸ“– **Ver guÃ­a completa de modelos:** [MODELS.md](MODELS.md)

## ğŸ§ª Pruebas

Antes de ejecutar la aplicaciÃ³n, puedes verificar que todo funciona:

```bash
# Prueba 1: Verificar instalaciÃ³n
python3 test_installation.py

# Prueba 2: Probar respuesta de Ollama
python3 test_ollama_response.py
```

## ğŸ’» Uso

Una vez iniciada la aplicaciÃ³n con `python -m streamlit run app.py`:

1. Abre tu navegador en `http://localhost:8501`
2. Usa la interfaz para:
   - Ver informaciÃ³n del cliente
   - Consultar estado de pedidos
   - Acceder a informaciÃ³n de envÃ­os
   - Ver polÃ­ticas de devoluciÃ³n
   - Obtener informaciÃ³n de garantÃ­as

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ app.py                # Interfaz Streamlit
â”œâ”€â”€ support_system.py     # Sistema de agentes principal
â”œâ”€â”€ ollama_manager.py     # GestiÃ³n del servidor y modelos Ollama
â”œâ”€â”€ install_ollama.py     # Instalador automÃ¡tico de Ollama
â”œâ”€â”€ requirements.txt      # Dependencias del proyecto
â””â”€â”€ README.md            # DocumentaciÃ³n
```

## ğŸ”„ Alternativa: Usar OpenAI API

Si prefieres usar OpenAI en lugar de Ollama local:

1. ObtÃ©n una API key de [platform.openai.com](https://platform.openai.com/api-keys)

2. Crea un archivo `.env`:
```
OPENAI_API_KEY=tu_api_key_aqui
USE_OPENAI=true
```

3. El sistema usarÃ¡ automÃ¡ticamente el modelo GPT-4o-mini de OpenAI

**Beneficios:**
- Sin descargas de modelos locales
- Inferencia mÃ¡s rÃ¡pida
- Rendimiento consistente
- Sin lÃ­mites de recursos

## ğŸ”§ InstalaciÃ³n Manual de Ollama

Si prefieres instalar Ollama manualmente:

```bash
# MÃ©todo recomendado (script oficial)
curl -fsSL https://ollama.com/install.sh | sh

# Iniciar servidor
ollama serve

# Descargar modelo (en otra terminal)
ollama pull llama3.2:1b
```

## âš ï¸ Consideraciones para Codespaces

- **LÃ­mites de recursos**: Los Codespaces tienen lÃ­mites de CPU/memoria que pueden afectar el rendimiento
- **Almacenamiento**: Los modelos se descargan al almacenamiento del Codespace
- **Red**: La descarga inicial del modelo requiere conexiÃ³n a internet
- **Persistencia**: Los modelos persisten entre sesiones pero pueden necesitar re-descarga si se reinicia el entorno

# Modificado de repositorio original como demo:
https://github.com/Croups/agentic-customer-support-system


